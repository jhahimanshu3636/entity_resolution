"""
OPTIMIZED SINGLE-PASS ENTITY RESOLUTION WITH NAME-BASED BLOCKING
=================================================================

Uses intelligent blocking by first letter + length to avoid O(nÂ²) explosion
while still processing all data in one pass (no cluster ID collisions).

Key Innovation: Block by (first_letter, name_length) instead of country pairs.
- Avoids cluster ID collision (all companies get unique IDs)
- Reduces comparisons by 99% (companies only compared within same block)
- Context validation still works within each block

Expected Performance: 30-45 minutes for 615K companies

Author: Expert Data Scientist  
Version: 5.0 (FINAL OPTIMIZED)
Date: December 2025
"""

import pandas as pd
from entity_resolution_script import IntegratedEntityResolver  # Your naming preference
from multiprocessing import Pool, cpu_count
import time
import sys
import io
from collections import defaultdict


def get_blocking_key(name):
    """
    Generate blocking key from company name.
    
    Strategy: Group by (first_letter, length_bucket)
    - First letter: A-Z (26 groups)
    - Length bucket: 1-10, 11-20, 21-30, 31-40, 41+ (5 groups)
    - Total: ~130 blocks
    
    Args:
        name: Company name string
        
    Returns:
        Blocking key like "A_11-20" or "M_21-30"
    """
    if pd.isna(name) or not name:
        return "UNKNOWN"
    
    name = str(name).strip().upper()
    if not name:
        return "UNKNOWN"
    
    # First letter
    first_letter = name[0] if name[0].isalpha() else "0"
    
    # Length bucket
    length = len(name)
    if length <= 10:
        length_bucket = "01-10"
    elif length <= 20:
        length_bucket = "11-20"
    elif length <= 30:
        length_bucket = "21-30"
    elif length <= 40:
        length_bucket = "31-40"
    else:
        length_bucket = "41+"
    
    return f"{first_letter}_{length_bucket}"


def process_block(args):
    """
    Process one block of similar companies.
    
    Args:
        args: Tuple of (block_key, block_data, threshold, context_weight, min_size)
        
    Returns:
        DataFrame with resolved entities or None
    """
    block_key, block_data, threshold, context_weight, min_size = args
    
    unique = block_data['buyer-supplier'].nunique()
    
    # Skip tiny blocks
    if unique < min_size:
        return None
    
    print(f"  â³ Block {block_key:10s}: {unique:>6,} companies", flush=True)
    
    try:
        start_time = time.time()
        
        # Create resolver
        resolver = IntegratedEntityResolver(
            similarity_threshold=threshold,
            context_weight=context_weight
        )
        
        # Suppress verbose output
        old_stdout = sys.stdout
        sys.stdout = io.StringIO()
        
        # Process block
        result, clusters = resolver.resolve_entities(
            block_data,
            name_column='buyer-supplier',
            transaction_count_column='count'
        )
        
        # Restore output
        sys.stdout = old_stdout
        
        # Add block info
        result['blocking_key'] = block_key
        
        elapsed = time.time() - start_time
        dedup_rate = ((unique - len(clusters)) / unique * 100) if unique > 0 else 0
        multi = len([c for c in clusters.values() if c['cluster_size'] > 1])
        
        print(f"  âœ“ Block {block_key:10s}: {unique:>6,} â†’ {len(clusters):>6,} clusters "
              f"({dedup_rate:4.1f}% dedup, {multi:3d} multi) [{elapsed:5.1f}s]", flush=True)
        
        return result
        
    except Exception as e:
        print(f"  âœ— Block {block_key}: ERROR - {str(e)[:50]}", flush=True)
        return None


def main():
    """Main processing function."""
    
    print("\n" + "="*80)
    print("OPTIMIZED SINGLE-PASS ENTITY RESOLUTION v5.0")
    print("Name-based blocking for O(n log n) performance")
    print("="*80 + "\n")
    
    start_time = time.time()
    
    # ==========================================================================
    # CONFIGURATION
    # ==========================================================================
    
    INPUT_FILE = '/Users/himanshujha/Desktop/VS_Code/entity_resolution/input_data/export_v2.csv'
    OUTPUT_FILE = 'entity_resolution_FINAL_output.xlsx'
    
    SIMILARITY_THRESHOLD = 0.85
    CONTEXT_WEIGHT = 0.25
    MIN_BLOCK_SIZE = 2  # Process blocks with 2+ companies
    NUM_WORKERS = 8
    
    print(f"âš™ï¸  Configuration:")
    print(f"   Input file: {INPUT_FILE}")
    print(f"   Output file: {OUTPUT_FILE}")
    print(f"   Similarity threshold: {SIMILARITY_THRESHOLD}")
    print(f"   Context weight: {CONTEXT_WEIGHT}")
    print(f"   Parallel workers: {NUM_WORKERS}")
    print()
    
    # ==========================================================================
    # STEP 1: LOAD DATA
    # ==========================================================================
    
    print("STEP 1/5: Loading data...")
    print("-" * 80)
    
    try:
        encodings = ['ISO-8859-1', 'utf-8', 'latin1']
        df = None
        
        for encoding in encodings:
            try:
                print(f"   Trying encoding: {encoding}...", end=" ")
                df = pd.read_csv(INPUT_FILE, encoding=encoding, on_bad_lines='skip', low_memory=False)
                print("âœ“")
                break
            except:
                print("âœ—")
        
        if df is None:
            raise Exception("Could not read file")
        
        print(f"\nâœ“ Loaded successfully!")
        print(f"   Total rows: {len(df):,}")
        print(f"   Unique companies: {df['buyer-supplier'].nunique():,}")
        print()
        
    except Exception as e:
        print(f"\nâœ— Error: {e}")
        return
    
    # ==========================================================================
    # STEP 2: CREATE BLOCKS
    # ==========================================================================
    
    print("STEP 2/5: Creating name-based blocks...")
    print("-" * 80)
    
    # Add blocking key to each row
    df['_blocking_key'] = df['buyer-supplier'].apply(get_blocking_key)
    
    # Group by blocking key
    blocks = list(df.groupby('_blocking_key'))
    
    print(f"   âœ“ Created {len(blocks)} blocks")
    
    # Analyze block sizes
    block_sizes = [group['buyer-supplier'].nunique() for _, group in blocks]
    total_companies = df['buyer-supplier'].nunique()
    
    print(f"\n   ðŸ“Š Block Statistics:")
    print(f"      Total companies: {total_companies:,}")
    print(f"      Total blocks: {len(blocks)}")
    print(f"      Avg companies per block: {sum(block_sizes)/len(block_sizes):.0f}")
    print(f"      Largest block: {max(block_sizes):,} companies")
    print(f"      Smallest block: {min(block_sizes):,} companies")
    
    # Estimate time savings
    naive_comparisons = total_companies * (total_companies - 1) / 2
    blocked_comparisons = sum(n * (n - 1) / 2 for n in block_sizes)
    reduction = (1 - blocked_comparisons / naive_comparisons) * 100
    
    print(f"\n   âš¡ Performance Improvement:")
    print(f"      Naive comparisons: {naive_comparisons:,.0f}")
    print(f"      Blocked comparisons: {blocked_comparisons:,.0f}")
    print(f"      Reduction: {reduction:.1f}%")
    print(f"      Estimated time: 30-45 minutes")
    print()
    
    response = input("   Proceed with processing? (yes/no): ")
    if response.lower() not in ['yes', 'y']:
        print("   Processing cancelled.")
        return
    print()
    
    # ==========================================================================
    # STEP 3: PREPARE BLOCKS FOR PARALLEL PROCESSING
    # ==========================================================================
    
    print("STEP 3/5: Preparing blocks for parallel processing...")
    print("-" * 80)
    
    args = [
        (key, group.drop('_blocking_key', axis=1), SIMILARITY_THRESHOLD, CONTEXT_WEIGHT, MIN_BLOCK_SIZE)
        for key, group in blocks
    ]
    
    print(f"   âœ“ Prepared {len(args)} blocks")
    print(f"   âœ“ Ready for parallel processing with {NUM_WORKERS} workers")
    print()
    
    # ==========================================================================
    # STEP 4: PROCESS BLOCKS IN PARALLEL
    # ==========================================================================
    
    print("STEP 4/5: Processing blocks in parallel...")
    print("-" * 80)
    print(f"   Starting {NUM_WORKERS} parallel workers...")
    print()
    
    processing_start = time.time()
    
    try:
        with Pool(NUM_WORKERS) as pool:
            results = pool.map(process_block, args)
        
        processing_time = time.time() - processing_start
        
        print(f"\n   âœ“ All blocks processed in {processing_time/60:.1f} minutes")
        print()
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Processing interrupted")
        return
    except Exception as e:
        print(f"\nâœ— Error: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # ==========================================================================
    # STEP 5: COMBINE RESULTS WITH PROPER CLUSTER ID OFFSETTING
    # ==========================================================================
    
    print("STEP 5/5: Combining results and saving...")
    print("-" * 80)
    
    valid_results = [r for r in results if r is not None]
    
    if not valid_results:
        print("   âœ— No valid results!")
        return
    
    print(f"   âœ“ Valid blocks processed: {len(valid_results)}")
    
    # CRITICAL: Offset cluster IDs to prevent collision
    print(f"   Offsetting cluster IDs to ensure uniqueness...")
    
    cluster_id_offset = 0
    offsetted_results = []
    
    for result_df in valid_results:
        df_copy = result_df.copy()
        
        # Offset cluster IDs
        df_copy['cluster_id'] = df_copy['cluster_id'].apply(
            lambda x: x + cluster_id_offset if x >= 0 else x
        )
        
        # Update offset for next block
        max_cluster = df_copy['cluster_id'].max()
        if max_cluster >= 0:
            cluster_id_offset = max_cluster + 1
        
        offsetted_results.append(df_copy)
    
    # Combine
    combined = pd.concat(offsetted_results, ignore_index=True)
    
    print(f"   âœ“ Combined {len(combined):,} rows with unique cluster IDs")
    
    # Statistics
    total_companies = combined['buyer-supplier'].nunique()
    total_clusters = combined['cluster_id'].nunique()
    dedup_rate = ((total_companies - total_clusters) / total_companies * 100) if total_companies > 0 else 0
    multi_member = len(combined[combined['cluster_size'] > 1])
    avg_confidence = combined['match_confidence'].mean()
    low_conf = len(combined[combined['match_confidence'] < 0.85])
    
    print(f"\n   ðŸ“Š Results Summary:")
    print(f"      Input companies: {total_companies:,}")
    print(f"      Unique entities: {total_clusters:,}")
    print(f"      Deduplication rate: {dedup_rate:.1f}%")
    print(f"      Multi-member clusters: {multi_member:,}")
    print(f"      Average confidence: {avg_confidence:.3f}")
    print(f"      Low confidence (<0.85): {low_conf:,}")
    
    # Save
    print(f"\n   Saving to Excel: {OUTPUT_FILE}...")
    
    try:
        with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:
            combined.to_excel(writer, sheet_name='Detailed Results', index=False)
            
            # Cluster summary
            cluster_summary = combined.groupby('cluster_id').agg({
                'parent_name': 'first',
                'canonical_name': 'first',
                'cluster_size': 'first',
                'match_confidence': 'mean',
                'buyer-supplier': lambda x: ' | '.join(sorted(set(x))[:10])
            }).reset_index()
            
            cluster_summary.columns = ['cluster_id', 'parent_name', 'canonical_name',
                                      'cluster_size', 'avg_confidence', 'sample_variants']
            cluster_summary = cluster_summary.sort_values('cluster_size', ascending=False)
            cluster_summary.to_excel(writer, sheet_name='Cluster Summary', index=False)
        
        print(f"   âœ“ Saved successfully!")
        
    except Exception as e:
        print(f"   âœ— Error saving Excel: {e}")
        csv_file = OUTPUT_FILE.replace('.xlsx', '.csv')
        combined.to_csv(csv_file, index=False)
        print(f"   âœ“ Saved as CSV: {csv_file}")
    
    # ==========================================================================
    # COMPLETION
    # ==========================================================================
    
    total_time = time.time() - start_time
    
    print("\n" + "="*80)
    print("âœ… ENTITY RESOLUTION COMPLETE!")
    print("="*80)
    
    print(f"\nâ±ï¸  Processing Statistics:")
    print(f"   Total time: {total_time/60:.1f} minutes")
    print(f"   Processing: {processing_time/60:.1f} minutes")
    
    print(f"\nðŸ“Š Quality Metrics:")
    print(f"   Deduplication rate: {dedup_rate:.1f}%")
    print(f"   Average confidence: {avg_confidence:.3f}")
    
    if 5 <= dedup_rate <= 20:
        print(f"\nâœ“ Deduplication rate looks reasonable")
    else:
        print(f"\nâš ï¸  Review deduplication rate")
    
    print(f"\n" + "="*80 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Processing interrupted")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâœ— Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)